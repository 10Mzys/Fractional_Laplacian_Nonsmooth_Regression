{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "import scipy\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x): # piecewise constant regression function (other piecewise constant functions will \n",
    "            # also be considered later, denoted by regnew1, regnew2 ,etc)\n",
    "    if 0<x<1:\n",
    "        return 1\n",
    "    if 1<=x<2:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta(x,y,eps): # the kernel function eta (other types of kernels will also be considered later)\n",
    "    if np.abs(x-y)<=eps:\n",
    "        return 1\n",
    "    if np.abs(x-y)>eps:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lap(n,X): # the unnormalized Laplacian matrix\n",
    "    W=np.zeros((n,n))\n",
    "    D=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            W[i,j]=eta(X[i],X[j],eps)\n",
    "        D[i,i]=np.sum(W[i,:])\n",
    "    return (D-W)/(n*eps**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs the sample size n\n",
    "# listn2 contains the sample sizes n considered\n",
    "# b indexes the repetitions and the results are averaged over 200 runs\n",
    "\n",
    "listn2=[500,600,700,800,900,1000]\n",
    "l2err=[]\n",
    "for n in listn2:\n",
    "    l2errb=np.zeros((200,1))\n",
    "    for b in range(200):\n",
    "        X=np.random.uniform(0,2,n)  # uniform design (later we will also consider other random designa including truncated normal design, etc.)\n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        eps=n**(-(1/2-0.001))\n",
    "        K=math.floor(n**(1/2-0.001))\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K,  which='LA') # calculate eigendecomposition\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=reg(X[i])+error[i]\n",
    "            FX[i]=reg(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        l2errb[b]=np.linalg.norm(FX-predict, axis=0)/n\n",
    "    l2err.append(np.mean(l2errb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the line with the theoretical slope to match the intercept\n",
    "\n",
    "b=np.log(l2err[0])-(-1/2)*np.log(500)\n",
    "theb=(-1/2)*np.log(listn2)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot: MSE vs n\n",
    "\n",
    "plt.scatter(np.log(listn),np.log(l2err))\n",
    "plt.plot(np.log(listn),np.log(l2err),\"-b\", label=\"PCR-FLE\")\n",
    "plt.plot(np.log(listn),theb,\"-r\", label=\"theoretical slope = -1/2\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Mean squre error\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('d=1, piecewise constant function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg2(x): # piecewise polynoimal regression function (other piecewise constant regression functions \n",
    "             # will also be considered later denoted by regnew1, regnew2 ,etc)\n",
    "    if 0<x<=1:\n",
    "        return x\n",
    "    if 1<x<=1.5:\n",
    "        return x**2\n",
    "    if 1.5<x<2:\n",
    "        return (x+x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs the sample size n\n",
    "# listn2 contains the sample sizes n considered\n",
    "# b indexes the repetitions and the results are averaged over 200 runs\n",
    "\n",
    "listn2=[500,600,700,800,900,1000]\n",
    "l2err2=[]\n",
    "for n in listn2:\n",
    "    l2errb2=np.zeros((200,1))\n",
    "    for b in range(200):\n",
    "        #X=np.random.uniform(0,2,n)\n",
    "        X=scipy.stats.truncnorm.rvs(0, 2, size = n) # truncated normal design\n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        eps=n**(-(1/2-0.001))\n",
    "        K=math.floor(n**(1/2-0.001))\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K,  which='LA') # calculate eigendecomposition\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=reg2(X[i])+error[i]\n",
    "            FX[i]=reg2(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        l2errb2[b]=np.linalg.norm(FX-predict, axis=0)/n\n",
    "    l2err2.append(np.mean(l2errb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the line with the theoretical slope to match the intercept\n",
    "\n",
    "\n",
    "b2=np.log(l2err2[0])-(-1/2)*np.log(500)\n",
    "theb2=(-1/2)*np.log(listn)+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(listn),np.log(l2err2))\n",
    "plt.plot(np.log(listn),np.log(l2err2),\"-b\", label=\"PCR-FLE\")\n",
    "plt.plot(np.log(listn),theb2,\"-r\", label=\"theoretical slope = -1/2\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Mean squre error\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('d=1, piecewise polynomial function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part involves the grid search of the tunning parameters\n",
    "\n",
    "# 1) grid search of K: the number of eigenvalues used\n",
    "\n",
    "n=100\n",
    "l2err3=[]\n",
    "for K in range(0,99,2):\n",
    "    l2errb3=np.zeros((200,1))\n",
    "    #X=scipy.stats.truncnorm.rvs(0, 2, size = n) \n",
    "    for b in range(200):\n",
    "        #X=np.random.uniform(0,2,n)\n",
    "        X=scipy.stats.truncnorm.rvs(0, 2, size = n) \n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        eps=n**(-(1/2-0.001))\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K+1,  which='LA')\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=reg2(X[i])+error[i]\n",
    "            FX[i]=reg2(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        l2errb3[b]=np.linalg.norm(FX-predict, axis=0)/n\n",
    "    l2err3.append(np.mean(l2errb3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(0,99,2),l2err3)\n",
    "plt.plot(range(0,99,2),l2err3,\"-b\")\n",
    "plt.xlabel(\"Number of eigenvectors K\")\n",
    "plt.ylabel(\"Mean squre error\")\n",
    "plt.title('d=1, piecewise polynomial function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part involves the grid search of the tunning parameters\n",
    "\n",
    "# 2) grid search of epsilon: the graph radius\n",
    "\n",
    "n=100\n",
    "l2err4=[]\n",
    "for N in range(25,499,2):\n",
    "    l2errb4=np.zeros((200,1))\n",
    "    #X=scipy.stats.truncnorm.rvs(0, 2, size = n)\n",
    "    for b in range(200):\n",
    "        X=scipy.stats.truncnorm.rvs(0, 2, size = n)\n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        eps=N/500\n",
    "        K=10\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K+1,  which='LA')\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            #Vk[:,j]=eigVectors[:,j]/np.sqrt((np.linalg.norm(eigVectors[:,j], axis=0))**2/n)\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=reg2(X[i])+error[i]\n",
    "            FX[i]=reg2(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        l2errb4[b]=np.linalg.norm(FX-predict, axis=0)/n\n",
    "    l2err4.append(np.mean(l2errb4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(10,99,2),l2err4)\n",
    "plt.plot(range(10,99,2),l2err4,\"-b\")\n",
    "plt.xlabel(\"Number of eigenvectors K\")\n",
    "plt.ylabel(\"Mean squre error\")\n",
    "plt.title('d=1, piecewise polynomial function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[i/500 for i in range(25,499,2)]\n",
    "plt.scatter(a,l2err4)\n",
    "plt.plot(a,l2err4,\"-b\")\n",
    "plt.xlabel(\"Graph radius\")\n",
    "plt.ylabel(\"Mean squre error\")\n",
    "plt.title('d=1, piecewise polynomial function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regnew1(x): # piecewise constant regression function\n",
    "    if 0<x<1:\n",
    "        return 1\n",
    "    if 1<=x<2:\n",
    "        return 0.5\n",
    "    if 2<=x<3:\n",
    "        return 2\n",
    "    if 3<=x<5:\n",
    "        return -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regnew2(x): # piecewise polynomial regression function\n",
    "    if 0<x<1:\n",
    "        return x\n",
    "    if 1<=x<2:\n",
    "        return 2*x**2+2\n",
    "    if 2<=x<3:\n",
    "        return -x+2\n",
    "    if 3<=x<5:\n",
    "        return 0.2*x**3-2*x-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of of fitted regression curve (using regnew1 as true regression fucntion)\n",
    "\n",
    "# listn: the sample size \n",
    "# eps and K are the tuning parameters optimized by double grid search\n",
    "# averpredict records the estimated value after 200 repetions\n",
    "\n",
    "listn=[1000]\n",
    "eps=beps\n",
    "K=bK\n",
    "for n in listn:\n",
    "    X=np.random.uniform(0,5,n)\n",
    "    averpredict=np.matrix(np.zeros((n,200)))\n",
    "    for b in range(200):        \n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        #eps=n**(-(1/2-0.001))\n",
    "        #K=math.floor(n**(1/2-0.001))\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K,  which='LA')\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=regnew1(X[i])+error[i]\n",
    "            FX[i]=regnew1(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        averpredict[:,b]=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averpredict records the estimated value after 200 repetions\n",
    "# xx used to plot the true regression function regnew1\n",
    "\n",
    "plt.scatter(X,np.array(np.mean(averpredict,axis=1)),label=\"PCR-FLE\")\n",
    "xx = np.linspace(0, 5, 1000)\n",
    "plt.plot(xx,[regnew1(xxx) for xxx in xx],\"-r\",label=\"True regression function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.title('d=1, piecewise constant function')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of fitted regression curve (using regnew2 as true regression function)\n",
    "\n",
    "# listn: the sample size \n",
    "# eps and K are the tuning parameters optimized by double grid search\n",
    "# averpredict records the estimated value after 200 repetions\n",
    "\n",
    "listn=[1000]\n",
    "eps=beps\n",
    "K=bK\n",
    "for n in listn:\n",
    "    X=np.random.uniform(0,5,n)\n",
    "    averpredict=np.matrix(np.zeros((n,200)))\n",
    "    for b in range(200):        \n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        #eps=n**(-(1/2-0.001))\n",
    "        #K=math.floor(n**(1/2-0.001))\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K,  which='LA')\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=regnew2(X[i])+error[i]\n",
    "            FX[i]=regnew2(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        averpredict[:,b]=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averpredict records the estimated value after 200 repetions\n",
    "# xx used to plot the true regression function regnew2\n",
    "\n",
    "plt.scatter(X,np.array(np.mean(averpredict,axis=1)),label=\"PCR-FLE\")\n",
    "xx = np.linspace(0, 5, 1000)\n",
    "plt.plot(xx,[regnew2(xxx) for xxx in xx],\"-r\",label=\"True regression function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.title('d=1, piecewise polynomial function')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following part again uses regnew1 and changes the sample size to n=100 and plots the fitted regression curve\n",
    "\n",
    "listn=[100]\n",
    "eps=beps\n",
    "K=bK\n",
    "for n in listn:\n",
    "    X=np.random.uniform(0,5,n)\n",
    "    averpredict=np.matrix(np.zeros((n,200)))\n",
    "    for b in range(200):        \n",
    "        error=np.random.normal(0, 1, size=n)\n",
    "        eigVal,eigVectors = scipy.sparse.linalg.eigsh(-Lap(n,X), K,  which='LA')\n",
    "\n",
    "        Vk=np.zeros((n,K))\n",
    "        for j in range(K):\n",
    "            #Vk[:,j]=eigVectors[:,j]/np.sqrt((np.linalg.norm(eigVectors[:,j], axis=0))**2/n)\n",
    "            Vk[:,j]=eigVectors[:,j]\n",
    "    \n",
    "        Y=np.zeros((n,1))\n",
    "        FX=np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            Y[i]=regnew1(X[i])+error[i]\n",
    "            FX[i]=regnew1(X[i])\n",
    "        predict=Vk@Vk.T@Y\n",
    "        averpredict[:,b]=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,np.array(np.mean(averpredict,axis=1)),label=\"PCR-FLE\")\n",
    "xx = np.linspace(0, 5, 1000)\n",
    "plt.plot(xx,[regnew1(xxx) for xxx in xx],\"-r\",label=\"True regression function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.title('d=1, piecewise constant function')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
